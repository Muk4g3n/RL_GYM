{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Concatenate,Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "from rl.memory import SequentialMemory,EpisodeParameterMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the environement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TRETEC\\anaconda3\\envs\\ai\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TRETEC\\anaconda3\\envs\\ai\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Ant-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the cartpole environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states : 27\n",
      "number of actions : 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of states : {states}\")\n",
    "print(f\"number of actions : {actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.77777014,  1.61724424,  0.81562214, -1.02381538,  0.32098166,\n",
       "       -1.49601233, -0.30379704, -0.36751761, -0.55325822, -2.01112154,\n",
       "       -0.37974534,  1.79045066,  0.96841525,  1.35483594,  0.49838011,\n",
       "       -0.22222846,  1.36946265,  0.92481802, -0.66038453, -1.29365243,\n",
       "        0.96494259,  0.39274802, -0.60784743, -0.81422861,  1.27714252,\n",
       "        1.09033436, -0.95454529])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8269655 , -0.03337483,  0.7987928 , -0.56918126,  0.07511768,\n",
       "        0.2451269 , -0.29642606, -0.8184176 ], dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Environement on random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 10\n",
    "# for episode in range(1,episodes+1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score=0\n",
    "\n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         _,reward,done,_ = env.step(action)\n",
    "#         score +=reward\n",
    "#         env.render()\n",
    "\n",
    "#     print(f\"Episode {episode}, score {score}\")\n",
    "\n",
    "# env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the training the neural network on the environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states,actions):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(1,states)),\n",
    "        Dense(128,activation=\"relu\"),\n",
    "        Dense(128,activation=\"relu\"),\n",
    "        Dense(128,activation=\"relu\"),\n",
    "        Dense(actions,activation=\"linear\"),\n",
    "        # Reshape((8,)),\n",
    "    ])\n",
    "    return model\n",
    "# model = Sequential([\n",
    "#     observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "\n",
    "# # Define your custom neural network model\n",
    "# x = Flatten()(observation_input)\n",
    "#     Flatten(input_shape=(1,states)),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(actions, activation='tanh')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_input = Input(shape=(actions,),name=\"action_input\")\n",
    "observation_input = Input(shape=(1,states),name=\"observation_input\")\n",
    "flattened_input = Flatten()(observation_input)\n",
    "x=Concatenate()([action_input,flattened_input])\n",
    "x=Dense(128)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Dense(128)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Dense(128)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Dense(1)(x)\n",
    "x=Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input,observation_input],outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = build_model(states,actions)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the memory\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "\n",
    "random_process = OrnsteinUhlenbeckProcess(size=actions,theta=.15,mu=0.,sigma=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = CEMAgent(model=build_model(), memory=memory, nb_actions=env.action_space.shape[0])\n",
    "# agent = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "#                nb_actions=actions, nb_steps_warmup=10,\n",
    "#                target_model_update=1e-2)\n",
    "agent = DDPGAgent(nb_actions=actions,actor=actor,critic=critic,critic_action_input=action_input\n",
    "                  ,memory=memory,nb_steps_warmup_critic=100,nb_steps_warmup_actor=100,random_process=random_process\n",
    "                  ,gamma=.99,target_model_update=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -2.4781\n",
      "20 episodes - episode_reward: -1181.938 [-8203.627, -135.823] - loss: 9.551 - mae: 1.487 - mean_q: -6.630 - reward_forward: -0.014 - reward_ctrl: -3.464 - reward_survive: 1.000 - x_position: -0.301 - y_position: -0.385 - distance_from_origin: 1.129 - x_velocity: -0.014 - y_velocity: -0.011 - forward_reward: -0.014\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -1.0538\n",
      "10 episodes - episode_reward: -1049.401 [-2890.300, -270.792] - loss: 2.516 - mae: 0.594 - mean_q: 1.631 - reward_forward: 0.002 - reward_ctrl: -2.056 - reward_survive: 1.000 - x_position: 0.150 - y_position: 0.169 - distance_from_origin: 0.509 - x_velocity: 0.002 - y_velocity: 0.006 - forward_reward: 0.002\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: -0.7774\n",
      "11 episodes - episode_reward: -813.666 [-1522.557, -182.648] - loss: 2.100 - mae: 0.527 - mean_q: 6.642 - reward_forward: -0.008 - reward_ctrl: -1.770 - reward_survive: 1.000 - x_position: -0.191 - y_position: 0.273 - distance_from_origin: 0.647 - x_velocity: -0.008 - y_velocity: 0.009 - forward_reward: -0.008\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.8778\n",
      "10 episodes - episode_reward: -877.826 [-2273.987, -202.635] - loss: 1.400 - mae: 0.498 - mean_q: 10.942 - reward_forward: 0.005 - reward_ctrl: -1.883 - reward_survive: 1.000 - x_position: 0.178 - y_position: -0.361 - distance_from_origin: 0.827 - x_velocity: 0.005 - y_velocity: -0.013 - forward_reward: 0.005\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.7193\n",
      "10 episodes - episode_reward: -715.985 [-1227.033, -246.182] - loss: 1.771 - mae: 0.499 - mean_q: 16.112 - reward_forward: 0.014 - reward_ctrl: -1.733 - reward_survive: 1.000 - x_position: 0.375 - y_position: -0.135 - distance_from_origin: 0.678 - x_velocity: 0.014 - y_velocity: -0.005 - forward_reward: 0.014\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -1.0703\n",
      "10 episodes - episode_reward: -1064.110 [-3946.383, -281.708] - loss: 1.589 - mae: 0.561 - mean_q: 21.466 - reward_forward: 0.016 - reward_ctrl: -2.087 - reward_survive: 1.000 - x_position: 0.687 - y_position: 0.356 - distance_from_origin: 1.068 - x_velocity: 0.016 - y_velocity: 0.007 - forward_reward: 0.016\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: -0.3914\n",
      "10 episodes - episode_reward: -411.822 [-1119.790, -35.713] - loss: 2.211 - mae: 0.544 - mean_q: 26.527 - reward_forward: 0.024 - reward_ctrl: -1.416 - reward_survive: 1.000 - x_position: 0.745 - y_position: 0.105 - distance_from_origin: 1.201 - x_velocity: 0.024 - y_velocity: 0.001 - forward_reward: 0.024\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: -0.9891\n",
      "10 episodes - episode_reward: -964.296 [-1538.186, 70.928] - loss: 1.546 - mae: 0.544 - mean_q: 30.329 - reward_forward: 0.019 - reward_ctrl: -2.008 - reward_survive: 1.000 - x_position: 0.578 - y_position: -0.472 - distance_from_origin: 0.983 - x_velocity: 0.019 - y_velocity: -0.011 - forward_reward: 0.019\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: -0.6726\n",
      "11 episodes - episode_reward: -608.594 [-1466.850, 60.191] - loss: 1.195 - mae: 0.577 - mean_q: 34.872 - reward_forward: 0.010 - reward_ctrl: -1.683 - reward_survive: 1.000 - x_position: 0.408 - y_position: -0.277 - distance_from_origin: 0.721 - x_velocity: 0.010 - y_velocity: -0.007 - forward_reward: 0.010\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.3991\n",
      "done, took 2183.413 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11e87007130>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "agent.fit(env,nb_steps=100000,visualize=False,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TRETEC\\anaconda3\\envs\\ai\\lib\\site-packages\\gym\\core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 802.422, steps: 1000\n",
      "Episode 2: reward: 603.981, steps: 1000\n",
      "Episode 3: reward: 823.452, steps: 1000\n",
      "Episode 4: reward: 815.575, steps: 1000\n",
      "Episode 5: reward: 85.326, steps: 128\n",
      "Episode 6: reward: 88.748, steps: 171\n",
      "Episode 7: reward: 824.280, steps: 1000\n",
      "Episode 8: reward: 537.195, steps: 1000\n",
      "Episode 9: reward: 727.240, steps: 1000\n",
      "Episode 10: reward: 659.091, steps: 1000\n",
      "596.7309379812964\n"
     ]
    }
   ],
   "source": [
    "results =agent.test(env,nb_episodes=10,visualize=True)\n",
    "print(np.mean(results.history[\"episode_reward\"]))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
